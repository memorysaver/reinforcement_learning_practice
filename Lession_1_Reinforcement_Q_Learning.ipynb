{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lession 1: Reinforcement Q-Learning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVN5E+JXrJbugrEYYC9O5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memorysaver/reinforcement_learning_practice/blob/main/Lession_1_Reinforcement_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 1: Reinforcement Q-Learning Practice\n",
        "\n",
        "This unit is to understand the basic knowlowdge and the training envionment framework `OpenAI Gym`. By practicing Q-learning algo, you can have a better understanding what value-based method is to solve rl problem and its limits that cannot solve `large observation space` problems.\n",
        "\n",
        "## Reinforcement Learning Concept\n",
        "- [the tutorial by thomassimonini](https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c) (ðŸ‘‰  Must Read!!)\n",
        "- [thomassimonini rl course github](https://github.com/simoninithomas/Deep_reinforcement_learning_Course) \n",
        "- [Huggface RL program](https://github.com/huggingface/deep-rl-class)(ðŸ‘‰  same author's latest tutorial with huggface)\n",
        "\n",
        "## Gym Tutorial\n",
        "- [Gym Tutorial](https://www.gymlibrary.ml/content/tutorials/) (ðŸ‘‰  Great Tutorials!!)\n",
        "\n",
        "## Q-learning\n",
        "- https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-1-2-3e8f5e764358\n",
        "- https://www.gocoder.one/blog/rl-tutorial-with-openai-gym\n",
        "- https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "\n",
        "## Reference Libraries\n",
        "- ðŸŽ® Environment: [OpenAI Gym](https://www.gymlibrary.ml/content/api/)\n",
        "\n",
        "- ðŸ“š RL-Library: [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)\n",
        "\n"
      ],
      "metadata": {
        "id": "pUEPUmHSrR2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZyuJRd-pR_n"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "!pip install cmake 'gym[atari]' scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "metadata": {
        "id": "vt0V-vc_JIl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.render()\n"
      ],
      "metadata": {
        "id": "52S1-QmgqReC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why the state space is 500? \n",
        "\n",
        "You'll also notice there are four (4) locations that we can pick up and drop off a passenger: R, G, Y, B or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates. Our illustrated passenger is in location Y and they wish to go to location R.\n",
        "\n",
        "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
        "\n",
        "So, our taxi environment has\n",
        "total possible states.\n",
        "\n",
        "[reference](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)"
      ],
      "metadata": {
        "id": "LAcr-8xnPg49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ],
      "metadata": {
        "id": "9cNydW_JqgRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros((state_space, action_space))\n",
        "print(Q)\n",
        "print(Q.shape)"
      ],
      "metadata": {
        "id": "s26i8m1kJDQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_episodes = 25000        # Total number of training episodes\n",
        "total_test_episodes = 100     # Total number of test episodes\n",
        "max_steps = 200               # Max steps per episode\n",
        "\n",
        "learning_rate = 0.01          # Learning rate\n",
        "gamma = 0.99                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.001            # Minimum exploration probability \n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "Tstb17TPJFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Q, state):\n",
        "  # if random number > greater than epsilon --> exploitation\n",
        "  if(random.uniform(0,1) > epsilon):\n",
        "    action = np.argmax(Q[state])\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "  \n",
        "  return action"
      ],
      "metadata": {
        "id": "8vzZBpMBKXkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        #\n",
        "        action = epsilon_greedy_policy(Q, state)\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        Q[state][action] = Q[state][action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(Q[new_state]) - Q[state][action])      \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state"
      ],
      "metadata": {
        "id": "_tCMuUH-KYdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "rewards = []\n",
        "\n",
        "frames = []\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "    for step in range(max_steps):\n",
        "        env.render()     \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(Q[state][:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        total_rewards += reward\n",
        "        \n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            #print (\"Score\", total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "metadata": {
        "id": "h40ja-yhKYyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nouQtvsxKv-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}